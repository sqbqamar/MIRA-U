# MIRA-U: Confidence-Weighted Semi-Supervised Learning for Skin Lesion Segmentation

PyTorch implementation of the MIRA-U framework for semi-supervised skin lesion segmentation with uncertainty-aware pseudo-labeling and hybrid CNN-Transformer architecture.

## ğŸ“‹ Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Dataset Preparation](#dataset-preparation)
- [Training](#training)
- [Prediction](#prediction)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Results](#results)
- [Troubleshooting](#troubleshooting)

## âœ¨ Features

- **Two-Stage Training Pipeline** (Algorithms 1 & 2)
  - Stage 1: Masked Image Modeling (MIM) pretraining with **80% masking ratio**
  - Stage 2: Semi-supervised learning with confidence-weighted pseudo-labels
- **Uncertainty-Aware Pseudo-Labeling** using Monte Carlo dropout (M=20 passes)
- **Hybrid CNN-Transformer Student** (21.3M parameters) with bidirectional cross-attention
- **Lightweight ViT Teacher** (2.1M parameters) for efficient pseudo-label generation
- **Confidence Weighting** with uncertainty-based filtering (Ï„áµ¤=0.15, Îº=0.5)
- **EMA Teacher Updates** (Î±=0.999) for stable pseudo-labels
- **Comprehensive Metrics** (DSC, IoU, Accuracy, Precision, Recall, Sensitivity, Specificity)
- Test Time Augmentation (TTA) support

## ğŸ”§ Installation

### Prerequisites
- Python 3.8+
- CUDA 11.0+ (for GPU support)
- PyTorch 1.12+

### Install Dependencies

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install albumentations opencv-python numpy tqdm einops
```

Or use the requirements file:

```bash
pip install -r requirements.txt
```

## ğŸ“ Dataset Preparation

### Supported Datasets
- **ISIC-2016** (primary training/testing)
- **PH2** (external validation)


### Directory Structure

Organize your dataset as follows:

```
data/
â””â”€â”€ ISIC2016/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ ISIC_0000001.jpg
    â”‚   â”œâ”€â”€ ISIC_0000002.jpg
    â”‚   â””â”€â”€ ...
    â””â”€â”€ masks/
        â”œâ”€â”€ ISIC_0000001_segmentation.png
        â”œâ”€â”€ ISIC_0000002_segmentation.png
        â””â”€â”€ ...
```

**Important Notes:**
- Images should be RGB color images (`.jpg` format)
- Masks should be binary grayscale images (`.png` format)
- Mask filename format: `{image_name}_segmentation.png`
- Dataset will be automatically split into 80% train / 20% test
- Labeled ratio (10%, 20%, 30%, 50%) is controlled in configuration

### Download ISIC-2016

```bash
# Download from official ISIC Archive
# https://challenge.isic-archive.com/data/#2016

# Expected dataset size:
# - Training: 900 images
# - Testing: 379 images (used for final evaluation)
```

## ğŸš€ Training

### Quick Start

```bash
# Train with default settings (50% labeled data)
python training.py
```

### Custom Configuration Using config.py

Choose from pre-configured settings:

```python
from config import Config, ISIC2016_50_Config, ISIC2016_10_Config

# Default: 50% labeled data
config = Config()

# Or use specific configuration
config = ISIC2016_10_Config()  # 10% labeled (90 images)
config = ISIC2016_20_Config()  # 20% labeled (180 images)
config = ISIC2016_30_Config()  # 30% labeled (270 images)
config = ISIC2016_50_Config()  # 50% labeled (360 images)

# Print configuration
print(config)
```

### Key Parameters 

```python
from config import Config

config = Config()

# ========== STAGE 1: MIM PRETRAINING (Algorithm 1) ==========
config.mim_epochs = 50              # E_pretrain = 50
config.mask_ratio = 0.80            # p = 0.80 (80% masking)
config.patch_size = 8               # 8Ã—8 patches
config.teacher_lr = 0.001           # Î· = 0.001
config.mim_batch_size = 16          # Batch size for MIM

# ========== STAGE 2: SSL TRAINING (Algorithm 2) ==========
config.train_epochs = 150           # E_train = 150
config.mc_samples = 20              # M = 20 (MC dropout passes)
config.tau_u = 0.15                 # Ï„áµ¤ = 0.15 (uncertainty threshold)
config.kappa = 0.5                  # Îº = 0.5 (confidence scale)
config.ema_decay = 0.999            # Î± = 0.999 (EMA decay)
config.ramp_up_epochs = 80          # Î²(t) ramp-up over 80 epochs
config.batch_size = 4               # Batch size for SSL
config.student_lr = 0.001           # Î· = 0.001

# ========== LOSS WEIGHTS (Algorithm 2) ==========
config.lambda_D = 0.5               # Dice loss weight
config.lambda_B = 0.5               # BCE loss weight
config.lambda_U = 1.0               # Unsupervised CE weight
config.lambda_C = 0.1               # Consistency loss weight
config.gamma = 0.01                 # Entropy regularization weight
```

### Training Process

The training consists of **two stages**:

#### **Stage 1: MIM Pretraining (Algorithm 1)**
- **Duration**: 50 epochs
- **Objective**: Teacher network learns context-rich representations
- **Method**: Masked Image Modeling with **80% masking ratio**
- **Loss**: L_MIM = (1/|M|) Î£ ||Ã_m - I_m||â‚ (L1 reconstruction loss)
- **Data**: Uses ALL available images (no labels required)
- **Output**: Pretrained teacher parameters Ï†*

```
Stage 1 Pipeline:
Input â†’ Patch Embedding (8Ã—8) â†’ Random Masking (80%) â†’ 
ViT Encoder (4 layers) â†’ Decoder (2 layers) â†’ RGB Reconstruction
```

#### **Stage 2: Semi-Supervised Training (Algorithm 2)**
- **Duration**: 150 epochs
- **Components**: Teacher (frozen encoder, active segmentation head) + Student
- **Pseudo-Labeling**: 
  - M=20 MC dropout passes on weakly augmented images
  - Uncertainty estimation: Ïƒáµ¢ = âˆšVar(predictions)
  - Confidence weights: wáµ¢ = exp(-Ïƒáµ¢/Îº)
  - Filtering: Keep only wáµ¢ â‰¥ Ï„áµ¤
  - Soft pseudo-labels: á»¹áµ¢ = wáµ¢ Â· Î¼Ì‚áµ¢
- **Student Training**: Hybrid CNN-Transformer with strong augmentation
- **EMA Updates**: Ï†* â† Î±Â·Ï†* + (1-Î±)Â·Î¸ every iteration
- **Loss Ramp-Up**: Î²(t) = exp(-5(1 - t/80)Â²) for t < 80, else Î²(t) = 1.0

```
Stage 2 Pipeline:
Labeled: Strong Aug â†’ Student â†’ L_sup (Dice + BCE)
Unlabeled: 
  - Weak Aug â†’ Teacher (MC dropout Ã—20) â†’ Pseudo-labels á»¹
  - Strong Aug â†’ Student â†’ L_unsup (Confidence-weighted CE)
  - Consistency loss between teacher and student predictions
Total Loss: L = L_sup + Î²(t)Â·L_unsup + Î³Â·L_ent
```

### Monitor Training

Training displays:
- **Stage 1**: L_MIM (reconstruction loss)
- **Stage 2**: L_sup, L_unsup, L_ent, Î²(t), learning rate
- Automatic best model saving
- Periodic checkpoints every 20 epochs

Example output:
```
Stage 1: MIM Pretraining
Epoch 50/50 | L_MIM: 0.0234 | LR: 0.000100

Stage 2: Semi-Supervised Training
Epoch 150/150 Summary:
  L_sup: 0.1234 | L_unsup: 0.0567 | L_ent: 0.0012
  Total Loss: 0.1813 | Î²(t): 1.000
  Val Dice: 0.9153
  
```

### Fast Debug Mode

For quick testing:

```python
from config import FastDebugConfig

config = FastDebugConfig()  # 2 MIM epochs, 5 SSL epochs
```

## ğŸ”® Prediction

### Evaluate on Test Set

Evaluate the trained model and generate comprehensive metrics:

```bash
python predict.py
```

This will:
1. Load the best trained model (`./checkpoints/student_best_dice.pth`)
2. Evaluate on the test set
3. Calculate all metrics (DSC, IoU, ACC, PRE, REC, SEN, SPE)
4. Save prediction visualizations (8 samples)
5. Generate metrics report (`./results/evaluation_metrics.txt`)



### Predict Single Image

Predict segmentation mask for a single image:

```python
from predict import MIRAUPredictor

config = {
    'checkpoint_path': './checkpoints/student_best_dice.pth',
    'image_size': 256,
    'base_channels': 32
}

predictor = MIRAUPredictor(config['checkpoint_path'], config)

# Predict with visualization
pred_mask = predictor.predict_single_image(
    image_path='path/to/image.jpg',
    save_path='path/to/output_mask.png',
    visualize=True  # Creates a 3-panel comparison image
)

print(f"Prediction shape: {pred_mask.shape}")
print(f"Value range: [{pred_mask.min():.3f}, {pred_mask.max():.3f}]")
```

### Batch Prediction

Predict segmentation masks for all images in a directory:

```python
from predict import MIRAUPredictor

config = {
    'checkpoint_path': './checkpoints/student_best_dice.pth',
    'image_size': 256,
    'base_channels': 32
}

predictor = MIRAUPredictor(config['checkpoint_path'], config)

# Process entire directory
predictor.batch_predict(
    image_dir='./data/ISIC2016/images/',
    output_dir='./predictions/',
    use_tta=False  # Set True for Test Time Augmentation
)
```

### Test Time Augmentation (TTA)

Improve prediction quality using TTA (averages predictions over multiple augmentations):

```bash
# Edit predict.py and set:
use_tta = True
```

Or in code:

```python
# Evaluate with TTA
metrics = predictor.evaluate(
    test_loader,
    use_tta=True,  # Enables 4 augmentations (original, hflip, vflip, both)
    save_results=True,
    output_dir='./results_tta'
)
```

**TTA Augmentations:**
- Original image
- Horizontal flip
- Vertical flip  
- Both flips

Final prediction = Average of all 4 predictions

### Custom Prediction Pipeline

For advanced use cases:

```python
from predict import MIRAUPredictor
from data_loader import get_test_augmentation
import cv2
import torch

# Initialize predictor
predictor = MIRAUPredictor('./checkpoints/student_best_dice.pth', config)

# Load and preprocess image
image = cv2.imread('image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

transform = get_test_augmentation(image_size=256)
transformed = transform(image=image)
image_tensor = transformed['image'].unsqueeze(0)

# Predict (choose one method)
pred = predictor.predict(image_tensor)           # Standard prediction
pred_tta = predictor.predict_with_tta(image_tensor)  # With TTA

# Post-process
pred_binary = (pred.cpu().numpy() > 0.5).astype(np.uint8) * 255
cv2.imwrite('output_mask.png', pred_binary.squeeze())
```

### Prediction Output Structure

After running predictions, you'll have:

```
results/
â”œâ”€â”€ evaluation_metrics.txt          # All metrics (DSC, IoU, ACC, etc.)
â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ sample_000.png             # Visualization: input | GT | soft | binary
â”‚   â”œâ”€â”€ sample_001.png
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ sample_007.png             # 8 sample predictions
```

Each visualization shows:
1. **Input Image**: Original RGB image
2. **Ground Truth**: True segmentation mask
3. **Prediction (Soft)**: Probability map [0, 1]
4. **Prediction (Binary)**: Thresholded mask (threshold=0.5)

## ğŸ“‚ Project Structure

```
mira-u/
â”œâ”€â”€ config.py                  # Configuration file (revised parameters)
â”œâ”€â”€ model.py                   # Model architectures (MIMTeacher + HybridStudent)
â”œâ”€â”€ data_loader.py             # Data loading and augmentation
â”œâ”€â”€ training.py                # Two-stage training pipeline
â”œâ”€â”€ predict.py                 # Prediction and evaluation (create this)
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ data/                      # Dataset directory
â”‚   â””â”€â”€ ISIC2016/
â”‚       â”œâ”€â”€ images/            # Raw images (*.jpg)
â”‚       â””â”€â”€ masks/             # Ground truth masks (*_segmentation.png)
â”‚
â”œâ”€â”€ checkpoints/               # Saved models
â”‚   â”œâ”€â”€ teacher_pretrained_best.pth        # Stage 1 best
â”‚   â”œâ”€â”€ teacher_pretrained_final.pth       # Stage 1 final
â”‚   â”œâ”€â”€ student_best_dice.pth              # Stage 2 best (by Dice)
â”‚   â”œâ”€â”€ student_best_loss.pth              # Stage 2 best (by loss)
â”‚   â”œâ”€â”€ student_final.pth                  # Stage 2 final
â”‚   â””â”€â”€ student_epoch_*.pth                # Periodic checkpoints
â”‚
â””â”€â”€ results/                   # Prediction outputs
    â”œâ”€â”€ predictions/
    â””â”€â”€ metrics.txt
```

## âš™ï¸ Configuration Details

### Key Hyperparameters 

| Parameter | Description | Value | Reference |
|-----------|-------------|-------|-----------|
| **Stage 1 (Algorithm 1)** |
| `mim_epochs` | Pretraining epochs | 50 | Algorithm 1 |
| `mask_ratio` | Masking ratio | 0.80 | 80% |
| `patch_size` | Patch size | 8 | 8Ã—8 patches |
| `mim_batch_size` | Batch size | 16 | Section III-E |
| **Stage 2 (Algorithm 2)** |
| `train_epochs` | SSL training epochs | 150 | Algorithm 2 |
| `mc_samples` (M) | MC dropout passes | 20 | Algorithm 2, line 15 |
| `tau_u` (Ï„áµ¤) | Uncertainty threshold | 0.15 | Algorithm 2, line 21 |
| `kappa` (Îº) | Confidence scale | 0.5 | Algorithm 2, line 19 |
| `ema_decay` (Î±) | EMA decay factor | 0.999 | Algorithm 2, line 42 |
| `ramp_up_epochs` | Î²(t) ramp-up duration | 80 | Algorithm 2, lines 28-32 |
| `batch_size` | Batch size | 4 | Section III-E |
| **Model Architecture** |
| `embed_dim` | ViT embedding dim | 256 | Teacher: 2.1M params |
| `depth` | Transformer layers | 4 | Lightweight ViT |
| `num_heads` | Attention heads | 4 | Multi-head attention |
| `base_channels` | CNN base channels | 32 | Student: 21.3M params |
| **Loss Weights** |
| `lambda_D` | Dice loss weight | 0.5 | Algorithm 2 |
| `lambda_B` | BCE loss weight | 0.5 | Algorithm 2 |
| `lambda_U` | Unsupervised CE weight | 1.0 | Algorithm 2 |
| `lambda_C` | Consistency weight | 0.1 | Algorithm 2 |
| `gamma` (Î³) | Entropy weight | 0.01 | Algorithm 2 |

### Loss Function Breakdown

#### Stage 1: MIM Pretraining (Algorithm 1)
```
L_MIM = (1/|M|) Î£_{mâˆˆM} ||Ã_m - I_m||â‚
```
Only reconstruction loss on masked patches.

#### Stage 2: Semi-Supervised Learning (Algorithm 2)

**Supervised Loss (Labeled Data):**
```
L_sup = Î»_D Â· L_Dice + Î»_B Â· L_BCE
```

**Unsupervised Loss (Unlabeled Data):**
```
L_unsup = (Î»_U Â· L_CE_conf + Î»_C Â· L_cons) Â· Î²(t)

where:
  L_CE_conf = (1/|U|) Î£ CE(Å·áµ¢, á»¹áµ¢)  [Confidence-weighted]
  L_cons = ||Å·_weak - Å·_strong||Â²    [Consistency]
  á»¹áµ¢ = wáµ¢ Â· Î¼Ì‚áµ¢                      [Soft pseudo-labels]
  wáµ¢ = exp(-Ïƒáµ¢/Îº)                   [Confidence weights]
```

**Entropy Regularization:**
```
L_ent = -Î£ Å· Â· log(Å·)
```

**Total Loss:**
```
L_total = L_sup + Î²(t) Â· L_unsup + Î³ Â· L_ent
```

**Ramp-Up Function Î²(t):**
```python
if t < 80:
    Î²(t) = exp(-5 * (1 - t/80)Â²)
else:
    Î²(t) = 1.0
```

## ğŸ“Š Results

### ISIC-2016 Dataset

#### Table 1: Performance with Different Label Fractions

| Label Fraction | DSC â†‘ | IoU â†‘ | ACC â†‘ | PRE â†‘ | REC â†‘ | SEN â†‘ | SPE â†‘ |
|----------------|-------|-------|-------|-------|-------|-------|-------|
| 10% (90 imgs)  | 0.8934 | 0.8156 | 0.9612 | 0.8856 | 0.9024 | 0.9024 | 0.9734 |
| 20% (180 imgs) | 0.9045 | 0.8342 | 0.9658 | 0.8934 | 0.9167 | 0.9167 | 0.9768 |
| 30% (270 imgs) | 0.9089 | 0.8423 | 0.9671 | 0.8989 | 0.9201 | 0.9201 | 0.9781 |
| **50% (360 imgs)** | **0.9153** | **0.8552** | **0.9703** | **0.9013** | **0.9243** | **0.9243** | **0.9812** |

#### Table 2: Cross-Dataset Generalization (Train: ISIC-2016, Test: PH2)

| Method | DSC â†‘ | IoU â†‘ | ACC â†‘ | PRE â†‘ | REC â†‘ |
|--------|-------|-------|-------|-------|-------|
| MIRA-U | **0.9130** | **0.8632** | **0.9384** | **0.9208** | **0.8691** |

### Model Complexity

| Component | Parameters | Description |
|-----------|------------|-------------|
| MIMTeacher (Ï†*) | 2.1M | Lightweight ViT (4 layers, 4 heads, 8Ã—8 patches) |
| HybridStudent (Î¸) | 21.3M | U-Net with CNN + Swin Transformer + Cross-Attention |
| **Total** | **23.4M** | Teacher + Student |

### Training Time (NVIDIA RTX 3090)

| Stage | Duration | Notes |
|-------|----------|-------|
| Stage 1 (MIM) | ~2 hours | 50 epochs, batch_size=16 |
| Stage 2 (SSL) | ~8 hours | 150 epochs, batch_size=4 |
| **Total** | **~10 hours** | For 50% labeled setting |

## ğŸ”¬ Algorithm Details

### Algorithm 1: MIM Pretraining

```
Input: Dataset D (all images), masking ratio p=0.80, epochs E=50
Output: Pretrained teacher parameters Ï†*

1: Initialize teacher Ï† with random weights
2: for epoch = 1 to E do
3:    for each batch {x} ~ D do
4:        # Patch embedding
5:        P = PatchEmbed(x)              // 8Ã—8 patches
6:        
7:        # Random masking with ratio p
8:        M, ids = RandomMask(P, p=0.80) // 80% masking
9:        
10:       # Forward pass through teacher
11:       Ã = Teacher_Ï†(M)                // Reconstruct RGB
12:       
13:       # Compute MIM loss
14:       L_MIM = (1/|M|) Î£ ||Ã_m - I_m||â‚
15:       
16:       # Update teacher
17:       Ï† â† Ï† - Î·âˆ‡_Ï† L_MIM
18:    end for
19: end for
20: Return Ï†*
```

### Algorithm 2: Semi-Supervised Student Training

```
Input: Labeled D_L, Unlabeled D_U, pretrained Ï†*, epochs E=150
Output: Trained student parameters Î¸*

1: Initialize student Î¸ with random weights
2: for epoch = 1 to E do
3:    # Compute ramp-up coefficient
4:    if epoch < 80 then
5:        Î²(t) = exp(-5(1 - t/80)Â²)
6:    else
7:        Î²(t) = 1.0
8:    end if
9:    
10:   for each batch do
11:       # ===== LABELED DATA =====
12:       (x_i, y_i) ~ D_L  [with strong augmentation]
13:       Å·_i = Student_Î¸(x_i)
14:       L_sup = Î»_DÂ·Dice(Å·_i, y_i) + Î»_BÂ·BCE(Å·_i, y_i)
15:       
16:       # ===== UNLABELED DATA =====
17:       x_j ~ D_U  [with strong augmentation]
18:       xÌ‚_j ~ D_U  [with weak augmentation]
19:       
20:       # Generate pseudo-labels with MC dropout (M=20)
21:       Î¼Ì‚_j, Ïƒ_j = MC_Dropout(Teacher_Ï†*, xÌ‚_j, M=20)
22:       
23:       # Compute confidence weights
24:       w_j = exp(-Ïƒ_j / Îº)  [Îº=0.5]
25:       
26:       # Filter by uncertainty threshold
27:       if w_j â‰¥ Ï„_u then  [Ï„_u=0.15]
28:           á»¹_j = w_j Â· Î¼Ì‚_j  [Soft pseudo-labels]
29:           Å·_j = Student_Î¸(x_j)
30:           
31:           # Confidence-weighted CE loss
32:           L_unsup = Î»_U Â· CE(Å·_j, á»¹_j)
33:           
34:           # Consistency loss
35:           Å·_j_weak = Student_Î¸(xÌ‚_j)
36:           L_cons = Î»_C Â· ||Å·_j - Å·_j_weak||Â²
37:       end if
38:       
39:       # Entropy regularization
40:       L_ent = Î³ Â· (-Î£ Å·_j Â· log(Å·_j))
41:       
42:       # Total loss
43:       L = L_sup + Î²(t)Â·(L_unsup + L_cons) + L_ent
44:       
45:       # Update student
46:       Î¸ â† Î¸ - Î·âˆ‡_Î¸ L
47:       
48:       # EMA update teacher (every iteration)
49:       Ï†* â† Î±Â·Ï†* + (1-Î±)Â·Î¸  [Î±=0.999]
50:   end for
51: end for
52: Return Î¸*
```

## ğŸ›  Troubleshooting

### Out of Memory Errors

**Problem**: CUDA out of memory during training

**Solutions**:
```python
# Option 1: Reduce batch size
config.batch_size = 2
config.mim_batch_size = 8

# Option 2: Reduce image size
config.image_size = 224

# Option 3: Reduce MC samples during training
config.mc_samples = 10  # Instead of 20

# Option 4: Enable mixed precision (if supported)
config.use_amp = True
```

### Poor Performance / Not Converging

**Problem**: Model achieves low Dice score (<0.80)

**Checklist**:
1. âœ“ Verify data split is correct
   ```bash
   python data_loader.py  # Run test function
   ```

2. âœ“ Check mask format (binary: 0 or 255)
   ```python
   import cv2
   mask = cv2.imread('mask.png', cv2.IMREAD_GRAYSCALE)
   print(f"Unique values: {np.unique(mask)}")  # Should be [0, 255]
   ```

3. âœ“ Ensure Stage 1 completed successfully
   ```bash
   ls checkpoints/teacher_pretrained_*.pth  # Should exist
   ```

4. âœ“ Verify loss weights are balanced
   ```python
   # Try adjusting if one loss dominates
   config.lambda_D = 0.5
   config.lambda_B = 0.5
   config.lambda_U = 1.0
   ```

5. âœ“ Check augmentation is working
   ```python
   # Visualize augmented samples
   from data_loader import get_strong_augmentation
   ```

### Slow Training Speed

**Problem**: Training takes too long

**Solutions**:
```python
# Option 1: Increase num_workers
config.num_workers = 8  # Match CPU cores

# Option 2: Enable pin_memory (already enabled)
# Already set in create_dataloaders()

# Option 3: Reduce MC samples (trade accuracy for speed)
config.mc_samples = 10  # During training only

# Option 4: Use smaller model (not recommended)
config.base_channels = 16  # Reduces student params
```

### Installation Issues

**Problem**: Package installation failures

**Solutions**:
```bash
# For albumentations
pip install albumentations==1.3.1

# For einops
pip install einops

# For CUDA/PyTorch issues
# Verify CUDA version
nvidia-smi
# Install matching PyTorch
pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118
```

### Checkpoint Loading Errors

**Problem**: Cannot load saved checkpoint

**Solutions**:
```python
# Check checkpoint contents
checkpoint = torch.load('checkpoints/student_best_dice.pth')
print(checkpoint.keys())  # Should include 'student_state_dict'

# Load with map_location for CPU/GPU compatibility
checkpoint = torch.load(
    'checkpoints/student_best_dice.pth',
    map_location='cpu'  # or 'cuda:0'
)
```

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{mira-u2024,
  title={MIRA-U: Confidence-Weighted Semi-Supervised Learning for Skin Lesion Segmentation},
  author={[Authors]},
  journal={[Journal]},
  year={2024}
}
```

## ğŸ“„ License

This implementation is for research purposes. Please refer to the original paper for usage guidelines.

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ Contact

For questions about the implementation:
- Review the code documentation (inline comments)
- Check the configuration file (`config.py`) for parameter explanations
- Refer to Algorithms 1 & 2 in the manuscript
- Open an issue for bugs or feature requests

## ğŸ”— Related Resources

- **ISIC Archive**: https://challenge.isic-archive.com/
- **PyTorch Documentation**: https://pytorch.org/docs/
- **Albumentations**: https://albumentations.ai/docs/


---

**Last Updated**: January 2025  
**Version**: 1.0 
